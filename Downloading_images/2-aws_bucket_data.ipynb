{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import gzip\n",
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Species Names selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = [\n",
    "    \"Cortaderia jubata\",\n",
    "    \"Cardiospermum grandiflorum\",\n",
    "    \"Heracleum sosnowskyi\",\n",
    "    \"Cenchrus setaceus\",\n",
    "    \"Ailanthus altissima\",\n",
    "    \"Lysichiton americanus\",\n",
    "    \"Hakea sericea\",\n",
    "    \"Lygodium japonicum\",\n",
    "    \"Microstegium vimineum\",\n",
    "    \"Heracleum mantegazzianum\",\n",
    "    \"Lespedeza cuneata\",\n",
    "    \"Triadica sebifera\",\n",
    "    \"Pueraria montana var. Lobata\",\n",
    "    \"Prosopis juliflora\",\n",
    "    \"Gunnera tinctoria\",\n",
    "    \"Baccharis halimifolia\",\n",
    "    \"Asclepias syriaca\",\n",
    "    \"Heracleum persicum\",\n",
    "    \"Ehrharta calycina\",\n",
    "    \"Andropogon virginicus\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Download CSV with photo ids in the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(instructions are from https://github.com/inaturalist/inaturalist-open-data/tree/main/Metadata/Download) <br><br>\n",
    "\n",
    "- Step 2: Download the photos.csv.gz file: <br>\n",
    "aws s3 --no-sign-request --region us-east-1 cp s3://inaturalist-open-data/photos.csv.gz photos.csv.gz\n",
    "\n",
    "- Step 3: Download the taxa.csv.gz file: <br>\n",
    "aws s3 --no-sign-request --region us-east-1 cp s3://inaturalist-open-data/taxa.csv.gz taxa.csv.gz \n",
    "\n",
    "- Step 4: Download the observations.csv.gz file: <br>\n",
    "aws s3 --no-sign-request --region us-east-1 cp s3://inaturalist-open-data/observations.csv.gz observations.csv.gz\n",
    "\n",
    "- Step 5: Unzip both files manually!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Get the taxa.csv file loaded and find the photo ids we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter photos for given taxon IDs\n",
    "def filter_photos_for_taxon_ids(photos_file_path, taxon_ids):\n",
    "    filtered_photos = []\n",
    "    with open(photos_file_path, 'r') as f:\n",
    "        header = f.readline().strip().split(',')\n",
    "        for line in f:\n",
    "            row = line.strip().split(',')\n",
    "            if len(row) >= 6 and int(row[5]) in taxon_ids:\n",
    "                filtered_photos.append(row)\n",
    "    return pd.DataFrame(filtered_photos, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded from existing taxons_data file.\n",
      "       taxon_id                name\n",
      "14447     48059  Prosopis juliflora\n"
     ]
    }
   ],
   "source": [
    "# Check if the taxons_data file exists\n",
    "if os.path.exists('taxons_data.parquet'):\n",
    "    # Load DataFrame from Parquet\n",
    "    taxons_data = pd.read_parquet('taxons_data.parquet')\n",
    "    print(\"DataFrame loaded from existing taxons_data file.\")\n",
    "else:\n",
    "    # Load the taxonomic data from the CSV file with tab delimiter\n",
    "    tax = pd.read_csv('./taxa.csv/taxa.csv', delimiter='\\t')\n",
    "    # Filter rows with species names from the list\n",
    "    filtered_taxa = tax[tax['name'].isin(species_list)]\n",
    "    # Extract a list of all taxon IDs and names from the filtered DataFrame\n",
    "    taxons_data = filtered_taxa[['taxon_id', 'name']]\n",
    "    # Combine taxon_ids and names into a DataFrame\n",
    "    combined_data = pd.DataFrame({'Taxon IDs': taxons_data['taxon_id'], 'Names': taxons_data['name']})\n",
    "    # Convert the DataFrame to a tab-separated string\n",
    "    taxons_data = combined_data.to_csv(sep='\\t', index=False)\n",
    "    # Save DataFrame to Parquet\n",
    "    taxons_data.to_parquet('taxons_data.parquet')\n",
    "    # Print the tab-separated string\n",
    "    print(\"DataFrame created and saved from taxons_data file.\")\n",
    "\n",
    "# Display the loaded DataFrame\n",
    "print(taxons_data.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Get photos from the IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to know which columns we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['photo_uuid\\tphoto_id\\tobservation_uuid\\tobserver_id\\textension\\tlicense\\twidth\\theight\\tposition']\n"
     ]
    }
   ],
   "source": [
    "with open('./photos.csv/photos.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    header = next(reader)\n",
    "    print(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a smaller subset of the photos file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read csv with dask:  0.05032920837402344 sec\n"
     ]
    }
   ],
   "source": [
    "from dask import dataframe as dd\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "dask_df = dd.read_csv('./photos.csv/photos.csv')\n",
    "end = time.time()\n",
    "print(\"Read csv with dask: \",(end-start),\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photo_uuid\\tphoto_id\\tobservation_uuid\\tobserver_id\\textension\\tlicense\\twidth\\theight\\tposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8d6b2534-d30a-47a8-bc1c-986a21817997\\t21213\\t7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6e8112fd-f703-4052-94da-b7cfc03ff3d4\\t21216\\t7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49141c2f-48b0-4671-9cee-fce6efd11822\\t21215\\t0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71090faa-9110-4df7-bb8f-af2415fe2e72\\t21214\\t5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92c703d0-20f1-4da9-af9c-2a6bdb6db53b\\t21217\\te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  photo_uuid\\tphoto_id\\tobservation_uuid\\tobserver_id\\textension\\tlicense\\twidth\\theight\\tposition\n",
       "0  8d6b2534-d30a-47a8-bc1c-986a21817997\\t21213\\t7...                                              \n",
       "1  6e8112fd-f703-4052-94da-b7cfc03ff3d4\\t21216\\t7...                                              \n",
       "2  49141c2f-48b0-4671-9cee-fce6efd11822\\t21215\\t0...                                              \n",
       "3  71090faa-9110-4df7-bb8f-af2415fe2e72\\t21214\\t5...                                              \n",
       "4  92c703d0-20f1-4da9-af9c-2a6bdb6db53b\\t21217\\te...                                              "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the photos_data file exists\n",
    "if os.path.exists('photos_data.parquet'):\n",
    "    print(\"DataFrame loaded from existing photos_data file.\")\n",
    "    # Load DataFrame from Parquet\n",
    "    photos_data = pd.read_parquet('photos_data.parquet')\n",
    "    print(\"DataFrame loaded from existing photos_data file.\")\n",
    "else:\n",
    "    print(\"DataFrame created and saved from taxons_data file.\")\n",
    "    # Load specific columns from the CSV file\n",
    "    columns_to_load = ['photo_id', 'observation_uuid']\n",
    "    chunk_size = 1000 \n",
    "    # Initialize an empty list to store chunks of data\n",
    "    photo_chunks = []\n",
    "    # Load the CSV file in chunks\n",
    "    for chunk in pd.read_csv('./photos.csv/photos.csv', sep='\\t', usecols=columns_to_load, chunksize=chunk_size):\n",
    "        photo_chunks.append(chunk)\n",
    "    photos_data = pd.concat(photo_chunks)\n",
    "    photos_data.head(2)\n",
    "    # Save DataFrame to Parquet\n",
    "    taxons_data.to_parquet('photos_data.parquet')\n",
    "    # Print the tab-separated string\n",
    "\n",
    "\n",
    "# Display the loaded DataFrame\n",
    "print(photos_data.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the observations_data file exists\n",
    "if os.path.exists('observations_data.parquet'):\n",
    "    # Load DataFrame from Parquet\n",
    "    observations_data = pd.read_parquet('observations_data.parquet')\n",
    "    print(\"DataFrame loaded from existing observations_data file.\")\n",
    "else:\n",
    "    # the observations zipped file is too large to unpack completely, we only take a small subset\n",
    "\n",
    "    with gzip.open('./observations.csv/observations.csv.gz', 'rt') as f_in:\n",
    "        with open('./observations.csv/observations_sample.csv', 'w') as f_out:\n",
    "            for i, line in enumerate(f_in):\n",
    "                if i >= 10000:  \n",
    "                    break\n",
    "                f_out.write(line)\n",
    "    observations = pd.read_csv('./observations.csv/observations_sample.csv', delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    # Extract a list of all taxon IDs and names from the filtered DataFrame\n",
    "    observations_data = observations[['taxon_id', 'observation_uuid']]\n",
    "    # Save DataFrame to Parquet\n",
    "    observations_data.to_parquet('observations_data.parquet')\n",
    "    # Print the tab-separated string\n",
    "    print(\"DataFrame created and saved from observations_data file.\")\n",
    "\n",
    "# Display the loaded DataFrame\n",
    "print(observations_data.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge and result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The list of observers and photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge photos_data and observations_data on observer_id\n",
    "# merged_data = pd.merge(photos_data, observations_data, on='observation_uuid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The list of the photos with the specific taxons i want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique taxon_ids from taxon_df\n",
    "# valid_taxon_ids = taxons_data['taxon_id']\n",
    "\n",
    "# Filter merged_data to contain only rows with taxon_ids present in valid_taxon_ids\n",
    "# filtered_data = merged_data[merged_data['taxon_id'].isin(valid_taxon_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the filtered_data DataFrame by 'taxon_id' and counting the number of 'photo_ids' per group\n",
    "# photo_count_per_taxon = filtered_data.groupby('taxon_id')['photo_id'].count()\n",
    "\n",
    "# Displaying the result\n",
    "# print(photo_count_per_taxon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, folder):\n",
    "    # Extract photo_id from the URL\n",
    "    photo_id = url.split('/')[-2]\n",
    "\n",
    "    # Ensure folder exists\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Download the image\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(os.path.join(folder, f'{photo_id}.jpg'), 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {photo_id}.jpg\")\n",
    "    else:\n",
    "        print(f\"Failed to download {photo_id}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image_urls(photo_ids):\n",
    "    # Construct S3 URLs for the images\n",
    "    s3_urls = [f'https://inaturalist-open-data.s3.amazonaws.com/photos/{photo_id}/original.jpg' for photo_id in photo_ids]\n",
    "    return s3_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list of photo_ids\n",
    "# photo_ids = filtered_data['photo_id']  \n",
    "\n",
    "# Fetch image URLs\n",
    "# image_urls = fetch_image_urls(photo_ids)\n",
    "\n",
    "# Choose how many images per species\n",
    "# images_per_species = 5  # Replace with your desired number\n",
    "\n",
    "# Download and save images\n",
    "# for url in image_urls:\n",
    "#     taxon_id = url.split('/')[-3]  # Extract taxon_id from the URL\n",
    "#     folder = os.path.join('photos', taxon_id)\n",
    "#     download_image(url, folder)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
